{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#概要\" data-toc-modified-id=\"概要-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>概要</a></span></li><li><span><a href=\"#2018-12-18\" data-toc-modified-id=\"2018-12-18-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>2018-12-18</a></span><ul class=\"toc-item\"><li><span><a href=\"#検索組織の機械学習実行基盤-|-リクルートテクノロジーズ　メンバーズブログ\" data-toc-modified-id=\"検索組織の機械学習実行基盤-|-リクルートテクノロジーズ　メンバーズブログ-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>検索組織の機械学習実行基盤 | リクルートテクノロジーズ　メンバーズブログ</a></span><ul class=\"toc-item\"><li><span><a href=\"#登場キーワードについての調査\" data-toc-modified-id=\"登場キーワードについての調査-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>登場キーワードについての調査</a></span><ul class=\"toc-item\"><li><span><a href=\"#DRONE-:-CI/CD-のパイプライン処理\" data-toc-modified-id=\"DRONE-:-CI/CD-のパイプライン処理-2.1.1.1\"><span class=\"toc-item-num\">2.1.1.1&nbsp;&nbsp;</span>DRONE : CI/CD のパイプライン処理</a></span></li><li><span><a href=\"#DigDag-:-機械学習処理システム向けのパイプライン処理ツール\" data-toc-modified-id=\"DigDag-:-機械学習処理システム向けのパイプライン処理ツール-2.1.1.2\"><span class=\"toc-item-num\">2.1.1.2&nbsp;&nbsp;</span>DigDag : 機械学習処理システム向けのパイプライン処理ツール</a></span></li><li><span><a href=\"#.dig-ファイルのサンプル集\" data-toc-modified-id=\".dig-ファイルのサンプル集-2.1.1.3\"><span class=\"toc-item-num\">2.1.1.3&nbsp;&nbsp;</span>.dig ファイルのサンプル集</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概要\n",
    "機械学習について調べた履歴を記録する。  \n",
    "Jupyter Notebook の便利な機能は、履歴作成に当たって大変助かる。(Table of Contents の自動生成など)\n",
    "まずは、細かい整理はせず、片っ端から記録するというスタイルで始めるとしよう。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018-12-18\n",
    "\n",
    "## 検索組織の機械学習実行基盤 | リクルートテクノロジーズ　メンバーズブログ\n",
    "  https://recruit-tech.co.jp/blog/2018/09/10/qass_ml/\n",
    "  \n",
    "* 機械学習システムのアーキテクチャ図があるので参考になる。\n",
    "    * ![アーキテクチャ](https://s3-ap-northeast-1.amazonaws.com/prod-rtc-blog/wp-content/uploads/2018/08/31193348/flya.png)\n",
    "* ソフトウェア資産の再利用性を高めるため、Dockerを活用している。\n",
    "    * Google Container Registry (GCR)に、コンテナを登録している。\n",
    "* Google Kubernetes Engine (GKE)の上に、\"DRONE\", \"DigDag\" といったアプリケーションを構築。\n",
    "* 機械学習処理の最終出力は、Google Cloud Storage(GCS)上に保存。\n",
    "* 処理フローの共通化のポイント\n",
    "    1. モデル部分は共通化しているが、\n",
    "    2. 前処理部分は、サービスごとに開発する\n",
    "    \n",
    "### 登場キーワードについての調査\n",
    "\n",
    "#### DRONE : CI/CD のパイプライン処理\n",
    "\n",
    "JenkinsやCircle CIと同じカテゴリのCI/CDツールだが、**Gitイベント** をトリガーにして、**Dockerコンテナ**内部でジョブを実行することに特化したツールである。  \n",
    "Dockerを利用したマイクロサービス化が普及するにつれて、DRONEの普及も進んでいる模様。  \n",
    "\n",
    "1. 対象Gitリポジトリのルートディレクトリにある **.drone.yml** を読み込んで、パイプライン処理を実行する。\n",
    "   .drone.ymlの例\n",
    "   ***\n",
    "```\n",
    "pipeline:\n",
    "  # ステップは上から順に実行されます（順序付きmap）。\n",
    "  hello:\n",
    "    # ステップ毎にイメージを指定\n",
    "    image: alpine:3.6\n",
    "    # Dockerコンテナ起動時のエントリポイントとしてコマンドが実行されます。\n",
    "    # 各コマンドは同じシェルで実行されます。\n",
    "    commands:\n",
    "      - echo 　\"Hello World\"\n",
    "```\n",
    "   ***\n",
    "\n",
    "参照: https://engineering.linecorp.com/ja/blog/go-oss-ci-tool-drone-replaces-jenkins/\n",
    "\n",
    "\n",
    "#### DigDag : 機械学習処理システム向けのパイプライン処理ツール\n",
    "\n",
    "機械学習処理システムは、処理フローが複雑化しやすく（特に前処理）、技術的負債の利子が危険レベルになる傾向がある。  \n",
    "処理フローを記述しやすく、把握しやすく管理できるのがDigDagとのこと。  \n",
    "以下のように、処理フローを階層的に記述しやすい。\n",
    "\n",
    "![DigDag処理フローイメージ](http://docs.digdag.io/_images/grouping-tasks.png)\n",
    "\n",
    "\n",
    "yml的な記述をする .dig ファイルに処理フローの定義を記述する。\n",
    "\n",
    ".dig の例\n",
    "***\n",
    "```text\n",
    "# daily_etl.dig\n",
    "schedule:\n",
    "  daily>: 00:05:00\n",
    " \n",
    "+foo:\n",
    "  _retry: 3\n",
    " \n",
    "  +get_foo_metrics1:\n",
    "    sh>: SESSION_TIME=${session_time} python elasticsearch_client.py foo metrics1 1day > foo_metrics1_daily.csv\n",
    "  +vaidate_foo_metrics1:\n",
    "    sh>: bash data_validator.sh foo_metrics1_daily.csv\n",
    "  +commit_foo_metrics1:\n",
    "    sh>: bash commit.sh foo_metrics1_daily.csv\n",
    "  +get_foo_metrics2:\n",
    "    sh>: SESSION_TIME=${session_time} python elasticsearch_client.py foo metrics2 1day > foo_metrics2_daily.csv\n",
    "  +vaidate_foo_metrics2:\n",
    "    sh>: bash data_validator.sh foo_metrics2_daily.csv\n",
    "  +commit_foo_metrics2:\n",
    "    sh>: bash commit.sh foo_metrics2_daily.csv\n",
    "\n",
    "```\n",
    "***\n",
    "\n",
    "学習処理を定義する .dig ファイルの例を示す。\n",
    "***\n",
    "**daily_train.dig**\n",
    "```\n",
    "# daily_train.dig\n",
    "schedule:\n",
    "  daily>: 00:10:00\n",
    " \n",
    "+wait_daily_etl:\n",
    "  require>: daily_etl\n",
    " \n",
    "+foo:\n",
    "  +train:\n",
    "    _parallel: true\n",
    " \n",
    "    +train_foo_metrics1:\n",
    "      sh>: python train.py foo metrics1 foo_metrics1_daily.csv\n",
    "    +train_foo_metrics2:\n",
    "      sh>: python train.py foo metrics2 foo_metrics2_daily.csv\n",
    " \n",
    "  +cross_validation:\n",
    "    _pararell: true\n",
    " \n",
    "    +cross_validation_foo_metrics1:\n",
    "      sh>: python cross_validation.py foo metrics1\n",
    "    +cross_validation_foor_metrics2:\n",
    "      sh>: python cross_validation.py foo metrics2\n",
    " \n",
    "_error:\n",
    "  sh>: bash notify.sh\n",
    "```\n",
    "***\n",
    "\n",
    "cross_validation.py は、過学習を起こしていたと判断すると、終了コード 1 を返すプログラムとなっている。  \n",
    "過学習が起きると \\_error が呼び出され、通知処理(notify.sh)が起動するようになっている。  \n",
    "\n",
    "\n",
    "週次の処理をする場合は、以下のような依存関係を前提とした .digファイルを記述する。  \n",
    "以下の.digファイルは、 requireで指定された daily_train.dig が7回実行されるのを待ってから、foo 以下の処理が実行される。\n",
    "\n",
    "***\n",
    "```\n",
    "# fine_tuning.dig\n",
    "schedule:\n",
    "  weekly>: Mon,01:05:00\n",
    " \n",
    "+wait_daily_train:\n",
    "  loop>: 7\n",
    "  _do:\n",
    "    require>: daily_train\n",
    "    session_time: ${moment(last_session_time).add(i, 'day').format()}\n",
    " \n",
    "+foo:\n",
    "  +train:\n",
    "    _parallel: true\n",
    " \n",
    "    +fine_tuning_foo_metrics1:\n",
    "      sh>: python fine_tuning.py foo metrics1\n",
    "    +fine_tuning_foo_metrics2:\n",
    "      sh>: python fine_tuning.py foo metrics2\n",
    "```\n",
    "***\n",
    "\n",
    "\n",
    "定期的にポーリングして、モデルの性能悪化を検出したら、すかさず通知したいときには、以下のような .digファイルを作ると良いという。  \n",
    "以下では5分ごとに predict.py を実行し、モデルの性能が悪化して exit_code = 1 となったら、_error がキックされ、通知処理(notify.sh)が実行される\n",
    "。  \n",
    "\n",
    "*** \n",
    "```\n",
    "# predict.dig\n",
    "schedule:\n",
    "  minutes_interval>: 5\n",
    " \n",
    "+foo:\n",
    "  +predict:\n",
    "    _parallel: true\n",
    " \n",
    "    +predict_foo_metrics1:\n",
    "      sh>: python predict.py foo metrics1\n",
    "    +predict_foo_metrics2:\n",
    "      sh>: python predict.py foo metrics2\n",
    " \n",
    "_error:\n",
    "  sh>: bash notify.sh\n",
    "```\n",
    "***\n",
    "\n",
    "#### .dig ファイルのサンプル集\n",
    "\n",
    "トレジャーデータの以下のサイトで紹介している。  \n",
    "https://github.com/treasure-data/digdag/tree/master/examples\n",
    "\n",
    "##### 参照リンク\n",
    "* [DigDag公式サイト](http://docs.digdag.io/index.html)\n",
    "* https://www.lifull.blog/entry/2017/09/20/151600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kaggle]",
   "language": "python",
   "name": "conda-env-kaggle-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
